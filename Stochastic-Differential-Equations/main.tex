\documentclass{article}
\usepackage{geometry}
\usepackage{amsmath, amsfonts, amsthm, amssymb}

\title{Exercises and Solutions for \textbf{An Introduction to Stochastic Differential Equations}}
\author{Zehao Dou}
\date{September 2021}

\begin{document}
\allowdisplaybreaks[4]
\maketitle

\begin{abstract}

\end{abstract}
\newpage
\paragraph{Problem 1} Show, using the formal manipulations for Ito's chain rule, that
\[Y(t) := e^{W(t)-\frac{t}{2}}\]
solves the stochastic differential equation
\begin{equation*}
\begin{cases}
dY &= YdW\\
Y(0) &= 1.
\end{cases} 
\end{equation*}
(Hint: If $X(t):=W(t)-\frac{t}{2}$, then $dX=-\frac{dt}{2}+dW$.)

\paragraph{Solution of Problem 1}
According to the Ito's chain rule: since function $X(t):=W(t)-\frac{t}{2}$ satisfies the equation $X(t)=-\frac{dt}{2}+dW$, for $Y(t)=\exp(X(t))$, it holds that:
\[dY=\left(e^{X}\cdot\left(-\frac{1}{2}\right)+\frac12\cdot e^{X}\right)dt+e^{X}dW= 0+YdW=YdW.\]
Also, it's easy to verify that $Y(0)=1$. Therefore, the solution
\[Y(t) := e^{W(t)-\frac{t}{2}}\]
actually solves the SDE we want. 

\paragraph{Problem 2} Show that, 
\[S(t)=s_0e^{\sigma W(t)+\left(\mu-\frac{\sigma^2}{2}\right)t}\]
solves
\begin{equation*}
\begin{cases}
dS &= \mu Sdt + \sigma SdW\\
S(0) &= s_0.
\end{cases} 
\end{equation*}

\paragraph{Solution of Problem 2}
Again we use the Ito's chain rule: for function $X(t):= W(t)+\frac{\mu-\sigma^2/2}{\sigma}t$, we have:
\[dX= dW + \frac{\mu-\sigma^2/2}{\sigma} dt.\]
Therefore, for function $S(t)=s_0 e^{\sigma\cdot X(t)}$, it holds that:
\begin{equation*}
\begin{aligned}
dS &= \left(\sigma s_0 e^{\sigma X(t)}\cdot \frac{\mu-\sigma^2/2}{\sigma} +\frac12\cdot \sigma^2 s_0 e^{\sigma X(t)}\right)dt + \sigma s_0 e^{\sigma X(t)}dW\\
&= \mu s_0 e^{\sigma X(t)}dt + \sigma s_0 e^{\sigma X(t)}dW = \mu Sdt+\sigma SdW.
\end{aligned}    
\end{equation*}
Also, it's easy to verify that $S(0)=s_0$, which comes to our conclusion. 
\paragraph{Problem 3} (1) Let $(\Omega, \mu, P)$ be a probability space and let $A_1\subseteq A_2\subseteq\ldots\subseteq A_n\subseteq\ldots$ be events. Show that 
\[P\left(\bigcup_{n=1}^{\infty}A_n\right)=\lim_{m\rightarrow \infty} P(A_m).\]
(Hint: Look at the disjoint events $B_n :=A_{n+1}-A_n$.)\\
~\\
(2) Likewise, show that if $A_1\supseteq A_2 \supseteq\ldots\supseteq A_n\supset\ldots$, then
\[P\left(\bigcap_{n=1}^{\infty}A_n\right)=\lim_{m\rightarrow\infty} P(A_m).\]

\paragraph{Solution of Problem 3}~\\
(1) Consider the disjoint events $B_n=A_{n+1}-A_n$, then we know that these events are disjoint since for $\forall m<n$, it holds that:
\[B_m=A_{m+1}-A_m\subseteq A_{m+1}\subseteq A_n, ~A_{n+1}-A_n =B_n\cap A_n = \varnothing,\]
so we can conclude that $B_m\cap B_n=\varnothing$, which means $\{B_n\}$ are disjoint events. Also, we have $\bigcup_{n=1}^{\infty}A_n=\bigcup_{n=0}^{\infty}B_n$. It is because for $\forall x\in \bigcup_{n=1}^{\infty}A_n$, there exists a smallest $k$ such that $x\in A_k\Rightarrow x\in A_k-A_{k-1}=B_k\subseteq\bigcup_{n=0}^{\infty}B_n$, which leads to
\[\bigcup_{n=1}^{\infty}A_n\subseteq\bigcup_{n=0}^{\infty}B_n.\]
On the other hand, for $\forall x\in \bigcup_{n=0}^{\infty}B_n$, there exists $k$ such that $x\in B_k\subseteq A_{k+1}\subseteq\bigcup_{n=1}^{\infty}A_n$, which leads to
\[\bigcup_{n=1}^{\infty}A_n\supseteq\bigcup_{n=0}^{\infty}B_n.\]
Therefore, we conclude that $\bigcup_{n=1}^{\infty}A_n=\bigcup_{n=0}^{\infty}B_n$. Also, notice that $A_m=B_0\cup B_1\cup\ldots B_{m-1}$, which leads to
\[P(A_m)=\sum_{n=0}^{m-1}P(B_m).\]
Here, we use the countable additivity of $P(\cdot)$ under disjoint events $\{B_n\}$. Finally, it holds that:
\begin{equation*}
\begin{aligned}
&P\left(\bigcup_{n=1}^{\infty}A_n\right) = P\left(\bigcup_{n=0}^{\infty}B_n\right) = \sum_{n=0}^{\infty}P(B_n) =\lim_{m\rightarrow\infty}\sum_{n=0}^{m-1}P(B_n) = \lim_{m\rightarrow\infty} P(A_m)
\end{aligned}    
\end{equation*}
In the final step, we use the fact that the sequence $p_m := \sum_{n=0}^{m-1}P(B_n)=P(A_m)$ is non-decreasing and upper bounded by 1, and therefore it converges. \\
~\\
(2) Similarly, we have $A_1^c\subseteq A_2^c\subseteq\ldots\subseteq A_n^c\subseteq\ldots$. By using the conclusion of (1), we have:
\[P\left(\bigcap_{n=1}^{\infty}A_n\right)=1-P\left(\bigcup_{n=1}^{\infty}A_n^c\right)=1-\lim_{m\rightarrow\infty}P(A_m^c)=\lim_{m\rightarrow\infty}P(A_m),\]
which comes to our conclusion.

\paragraph{Problem 4} Let $\Omega$ be any set and $\mathcal A$ any collections of subsets of $\Omega$. Show that there exists a unique smallest $\sigma$-algebra $\mu$ of subsets of $\Omega$ containing $\mathcal A$. We call $\mu$ the $\sigma$-algebra generated by $\mathcal A$. \\
(Hint: Take the intersection of all the $\sigma$-algebras containing $\mathcal A$. )

\paragraph{Solution of Problem 4} Obviously, there exists a $\sigma$-algebra containing $\mathcal A$ (for example, $2^{\Omega}$ itself). Now, we take the intersection of all the $\sigma$-algebras containing $\mathcal A$, denoted by $\mu$. In order to prove that there exists a unique smallest $\sigma$-algebra $\mu$ of subsets of $\Omega$ containing $\mathcal A$, we only need to prove that the intersection of all $\sigma$-algebras is also a $\sigma$-algebra of $\Omega$ containing $\mathcal A$. On one hand, since all the $\sigma$-algebras considered by us contain $\mathcal A$, so their intersection $\mu$ also contains $\mathcal A$. On the other hand,
\begin{itemize}
\item For all $\sigma$-algebra $S$ that contains $\mathcal A$, we have $\varnothing \in S$ by the definition of $\sigma$-algebra. Therefore, the intersection holds $\varnothing\in \mu$ obviously. 
\item For $\forall A\in\mu$, we have $A\in S$ for any $\sigma$-algebra $S$ that contains $\mathcal A$, so it holds that $A^c\in S$. Therefore, we have $A^c\in\mu$. 
\item For $A_1, A_2, \ldots, \in\mu$, we know that for any $\sigma$-algebra $S$ that contains $\mathcal A$, $\bigcup_{i=1}^{+\infty} A_i\in S$. Therefore:
\[\bigcup_{i=1}^{+\infty} A_i\in \mu.\]
\end{itemize}
After combining all the items above, we know that $\mu$ is also a $\sigma$-algebra of $\Omega$ containing $\mathcal A$, which comes to our conclusion. 

\paragraph{Problem 5} Show that if $A_1, A_2,\ldots, A_n$ are events, then
\begin{equation*}
\begin{aligned}
P\left(\bigcup_{i=1}^{n}A_i\right) &= \sum_{i=1}^{n} P(A_i) -\sum_{1\leqslant i<j\leqslant n}P(A_i\cap A_j)\\
&~~~+ \sum_{1\leqslant i<j<k\leqslant n} P(A_i\cap A_j\cap A_k)\\
&~~~-\ldots+(-1)^n P(A_1\cap A_2\cap\ldots\cap A_n).
\end{aligned}    
\end{equation*}
(Hint: Do the case $n=2$ first and then the general case by induction.)
 
\paragraph{Solution of Problem 5} We use the method of induction. When $n=2$, we know that:
\[P(A\cup B)=P(A)+P((A\cup B)-A)=P(A)+P(B-(A\cap B))=P(A)+P(B)-P(A\cap B).\]
Suppose the condition holds for $n$, then for $n+1$, we have:
\begin{align}
&~~P(A_1\cup A_2\cup\ldots\cup A_{n+1}) = P((A_1\cup A_2\cup\ldots\cup A_n)\cup A_{n+1})\notag\\
&= P(A_1\cup A_2\cup\ldots\cup A_n)+P(A_{n+1})-P((A_1\cup A_2\cup\ldots\cup A_n)\cap A_{n+1})\notag\\
&= P(A_{n+1})+\sum_{i=1}^{n} P(A_i) -\sum_{1\leqslant i<j\leqslant n}P(A_i\cap A_j)+\ldots+(-1)^n P(A_1\cap A_2\cap\ldots\cap A_n)\notag\\
&~~-P\left((A_1\cap A_{n+1})\cup (A_2\cap A_{n+1})\cup\ldots\cup (A_n\cap A_{n+1})\right)\notag\\
&= P(A_{n+1})+\sum_{i=1}^{n} P(A_i) -\sum_{1\leqslant i<j\leqslant n}P(A_i\cap A_j)+\ldots+(-1)^n P(A_1\cap A_2\cap\ldots\cap A_n)\notag\\
&~~-\sum_{i=1}^{n} P(A_i\cap A_{n+1})+\sum_{1\leqslant i<j\leqslant n}P((A_i\cap A_{n+1})\cap(A_j\cap A_{n+1}))-\ldots\notag\\
&~~-(-1)^n P((A_1\cap A_{n+1})\cap (A_2\cap A_{n+1})\cap\ldots\cap (A_n\cap A_{n+1}))\notag\\
&= \sum_{i=1}^{n+1} P(A_i) -\sum_{1\leqslant i<j\leqslant n+1}P(A_i\cap A_j)+\ldots+(-1)^{n+1} P(A_1\cap A_2\cap\ldots\cap A_{n+1})\notag,
\end{align}    
which completes the induction and comes to our conclusion. 

\paragraph{Problem 6} Let $X=\sum_{i=1}^{k}a_i\chi_{A_i}$ be a simple random variable, where the real numbers $a_i$ are distinct, the events $A_i$ are pairwise disjoint, and $\Omega=\bigcup_{i=1}^{k}A_i$. Let $\mu(X)$ be the $\sigma$-algebra generated by $X$.\\
(1) Describe precisely which sets are in $\mu(X)$.\\
(2) Suppose the random variable $Y$ is $\mu(X)$-measurable. Show that $Y$ is constant on each set $A_i$.\\
(3) Show that therefore $Y$ can be written as a function of $X$. 

\paragraph{Solution of Problem 6}~\\
(1) Since the real numbers $a_i$ are distinct, we can assume that
\[a_1<a_2<\ldots<a_k\]
without loss of generality. We are going to prove that $\mu(X)=\sigma(\{A_i:~i\in[k]\})$ which is the $\sigma$-algebra generated by the pairwise disjoint events $A_i$. On one hand, notice that: $\forall i\in[k]$,
\[A_i=\{X=a_i\}\in\mu(X)~\Rightarrow~\sigma(\{A_i:~i\in[k]\}\subseteq \mu(X).\]
On the other hand, for any $B\in\mathcal B(\mathbb{R})$, we have:
\[\{X\in B\}=\bigcup_{a_i\in B}A_i \in\sigma(\{A_i:~i\in[k]\}),\]
which leads to:
\[\mu(X)=\sigma\left(\{X\in B\}\right)\subseteq \sigma(\{A_i:~i\in[k]\}).\]
To sum up, $\mu(X)=\sigma(\{A_i:~i\in[k]\})$, which means all the sets in $\mu(X)$ are $\bigcup_{i\in S}A_i$ for some $S\subseteq [k]$. 

~\\
(2) Random variable $Y$ is $\mu(X)$-measurable. Then, for any $B\in\mathcal B(\mathbb{R})$, it holds that event $\{Y\in B\}\in\mu(X)$. From the conclusion of (1), we know that 
\[\mu(X)=\left\{\bigcup_{i\in S}A_i:~S\subseteq [k]\right\}.\]
Now we suppose that $Y$ is not constant on each set $A_i$. Without loss of generality, we assume $Y$ is not constant on $A_1$, then there exists $x,y\in A_1$ such that $Y(x)<Y(y)$. Consider the event $E = \{Y=Y(x)\}\in\mu(X)$, then $x\in E$ but $y\notin E$. Since $E$ must be the union of some $A_i$-s, from $x\in E$, we know that $A_1\subseteq E$. From $y\notin E$, we know that $A_1\cap E=\varnothing$, which contradict with each other. Therefore, $Y$ must be constant on each set $A_i$.

~\\
(3) According to the conclusion of (2), we can assume that $Y$ equals to constant $b_i$ on the set $A_i$. Then:
\[Y=\sum_{i=1}^{k}b_i\chi_{A_i}.\]
Therefore, $Y$ can be written as $f(X)$ where $f(a_i)=b_i$ for $\forall i\in[k]$, which comes to our conclusion. 


\paragraph{Problem 7} Verify:
\[\int_{-\infty}^{\infty}e^{-x^2}dx=\sqrt{\pi},~~~\frac{1}{\sqrt{2\pi\sigma^2}}\int_{-\infty}^{\infty}xe^{-\frac{(x-m)^2}{2\sigma^2}}dx=m,\]
\[\frac{1}{\sqrt{2\pi\sigma^2}}\int_{-\infty}^{\infty}(x-m)^2e^{-\frac{(x-m)^2}{2\sigma^2}}dx=\sigma^2.\]

\paragraph{Solution of Problem 7} For the first conclusion, we consider the following equation: denote $S=\int_{-\infty}^{\infty}e^{-x^2}dx$, then:
\begin{equation*}
\begin{aligned}
S^2&=\int_{-\infty}^{\infty}e^{-x^2}dx \cdot\int_{-\infty}^{\infty}e^{-y^2}dy = \int_{\mathbb{R}^2}e^{-(x^2+y^2)}dxdy\\
& = \int_{0}^{+\infty}\int_{-\pi}^{\pi} e^{-r^2}r\cdot drd\theta = \pi\cdot \int_{0}^{+\infty}e^{-r^2}\cdot 2rdr = \pi\cdot \int_{0}^{+\infty}e^{-r^2}dr^2 = \pi.
\end{aligned}    
\end{equation*}
It is obvious that $S>0$, therefore $S=\sqrt{\pi}$. For the following two equations, let $x=m+\sqrt{2}\sigma y$, then:
\begin{equation*}
\begin{aligned}
&\frac{1}{\sqrt{2\pi\sigma^2}}\int_{-\infty}^{\infty}xe^{-\frac{(x-m)^2}{2\sigma^2}}dx = \frac{1}{\sqrt{\pi}}\int_{-\infty}^{\infty}(m+\sqrt{2}\sigma y)e^{-y^2}dy\\
=~&\frac{m}{\sqrt{\pi}}\int_{-\infty}^{\infty}e^{-y^2}dy+\frac{\sqrt{2}\sigma}{\sqrt{\pi}}\int_{-\infty}^{\infty}ye^{-y^2}dy = m+0=m.
\end{aligned}    
\end{equation*}
Here, we use the conclusion of $\int_{-\infty}^{\infty}e^{-y^2}dy=\sqrt{\pi}$ and the fact that $ye^{-y^2}$ is an odd function, whose integral over $\mathbb{R}$ must be 0. 
\begin{equation*}
\begin{aligned}
&\frac{1}{\sqrt{2\pi\sigma^2}}\int_{-\infty}^{\infty}(x-m)^2e^{-\frac{(x-m)^2}{2\sigma^2}}dx = \frac{1}{\sqrt{\pi}}\int_{-\infty}^{\infty}2\sigma^2 y^2 e^{-y^2}dy\\
=~&\frac{\sigma^2}{\sqrt{\pi}}\int_{-\infty}^{\infty}y\cdot 2ye^{-y^2}dy = \frac{\sigma^2}{\sqrt{\pi}}\int_{-\infty}^{\infty}-y de^{-y^2} = \frac{\sigma^2}{\sqrt{\pi}}\cdot \left(-ye^{-y^2}\Bigg|_{-\infty}^{\infty}+\int_{-\infty}^{\infty}e^{-y^2}dy\right)\\
=~&\frac{\sigma^2}{\sqrt{\pi}}\cdot\sqrt{\pi}=\sigma^2.
\end{aligned}    
\end{equation*}
Here, we again use the equation $\int_{-\infty}^{\infty}e^{-y^2}dy=\sqrt{\pi}$.


\paragraph{Problem 8} Suppose $A$ and $B$ are independent events in some probability space. Show that $A^c$ and $B$ are independent. Likewise, show that $A^c$ and $B^c$ are independent. 

\paragraph{Solution of Problem 8} Since $A$ and $B$ are independent events, we have:
\[P(A\cap B)=P(A)P(B).\]
Therefore, we have the following two equations:
\[P(A^c\cap B)=P(B)-P(A\cap B)=P(B)-P(A)P(B)=(1-P(A))P(B)=P(A^c)P(B).\]
\begin{equation*}
\begin{aligned}
P(A^c\cap B^c) &= 1-P(A\cup B)=1-P(A)-P(B)+P(A\cap B)=1-P(A)-P(B)+P(A)P(B)\\
&=(1-P(A))(1-P(B))=P(A^c)\cdot P(B^c).
\end{aligned}    
\end{equation*}
These two equations conclude that $A^c$ and $B$ are independent. Also, $A^c$ and $B^c$ are independent. 


\paragraph{Problem 9} Suppose we have three cards: one is red on both sides, one is red on one side and white on the other side, and one is white on both sides. \\
(1) Pick a card and then one of its sides at random. What is the probability it is red?\\
(2) Given that the side of the card is red, what is the probability that the other side is red?

\paragraph{Solution of Problem 9}~\\
(1) We denote R as red and W as white, then:
\[P(R)=P(RR)\cdot 1+P(RB)\cdot \frac12 + P(BB)\cdot 0 = \frac13+\frac16=\frac12.\]
The probability of red side is $\frac12$. 

~\\
(2) By using Bayes' formula, we have:
\[P(RR|R)=\frac{P(RR)\cdot P(R|RR)}{P(RR)\cdot P(R|RR)+P(RB)\cdot P(R|RB)+P(BB)\cdot P(R|BB)}=\frac{1/3}{1/3+1/6+0}=\frac23.\]
The probability for the other side to be red is $\frac23$.




\paragraph{Problem 10} Suppose that $A_1, A_2,\ldots, A_m$ are disjoint events, each of positive probability, such that $\Omega=\bigcup_{j=1}^{m} A_j$. Prove Bayes' formula:
\[P(A_k|B)=\frac{P(B|A_k)P(A_k)}{\sum_{j=1}^{m}P(B|A_j)P(A_j)}\]
for $k=1,2,\ldots, m$ provided $P(B)>0$.

\paragraph{Solution of Problem 10} Since $A_1, A_2, \ldots, A_m$ all have positive probabilities, so:
\[P(B|A_j)P(A_j)=P(B\cap A_j).\]
Also, they are disjoint and $\bigcup_{j=1}^{m}A_j=\Omega$, which leads to $\{B\cap A_j\}_{j\in[m]}$ are also disjoint and $\bigcup_{j=1}^{m}(B\cap A_j)=B\cap \Omega=B$. Therefore:
\[\frac{P(B|A_k)P(A_k)}{\sum_{j=1}^{m}P(B|A_j)P(A_j)}=\frac{P(B\cap A_k)}{\sum_{j=1}^{m}P(B\cap A_j)}=\frac{P(B\cap A_k)}{P(B)}=P(A_k|B),\]
which comes to our conclusion. Here, we used the additivity of probability measures.


\end{document}
